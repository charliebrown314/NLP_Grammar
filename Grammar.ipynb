{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Jess_Joe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charliebrown314/NLP_Grammar/blob/main/Grammar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrDKzJLiC0a0"
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk import grammar, parse, Expression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qPWwkCHz0k"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TjmiN9RtQre"
      },
      "source": [
        "#Define model\n",
        "\n",
        "# Mostly Joe, some Jess #\n",
        "\n",
        "v = \"\"\"\n",
        "Tom => t\n",
        "Robin => r\n",
        "Mia => mia\n",
        "Sam => s\n",
        "Jack => j\n",
        "Andrew => a\n",
        "Dave => d\n",
        "Jess => je\n",
        "Mike => m\n",
        "Pumpkin => p\n",
        "#This is my cats name so thats why its a proper name :) \n",
        "Kate => k\n",
        "Lance => l\n",
        "Sheba => sh\n",
        "Bella => b\n",
        "Booker => bo\n",
        "Leon => le\n",
        "Claire => c\n",
        "Joe => jo\n",
        "Felix => fefe\n",
        "Tim => t\n",
        "Mary => ma\n",
        "Alex => al\n",
        "unknown => uk\n",
        "corner => {cor}\n",
        "A blue Sedan => se\n",
        "A black Sedan => bs\n",
        "The dealership => deal\n",
        "sedan => {se, bs}\n",
        "blue => {se}\n",
        "car => {se, bs}\n",
        "customer => {cu,st}\n",
        "cat => {p}\n",
        "tall => {j}\n",
        "skinny => {j}\n",
        "man => {j,d,l,bo,jo,t}\n",
        "boy => {s,a,m,le,f,al}\n",
        "woman => {mia,r,k}\n",
        "girl => {b,c,girl}\n",
        "sailor => {j}\n",
        "restaurant => {re}\n",
        "leave => {(j,re),(l,re),(m,re),(t,re),(cu,deal)}\n",
        "sneeze => {l,m,j}\n",
        "interrupt => {(r,sail)(j,r)}\n",
        "moustache => {mou}\n",
        "see => {(t,p)(mia,p),(ma,fefe),(t,se)}\n",
        "around => {(deal,cor)}\n",
        "buy => {(s,se,le),(s,bs),(s,se),(r,se)}\n",
        "in => {(bs,deal)}\n",
        "dealership => {deal}\n",
        "black => {bs}\n",
        "brochure => {br}\n",
        "insult => {(cu,uk)}\n",
        "give => {(mia,br,cu)}\n",
        "drunk => {j}\n",
        "with => {(j,mou)}\n",
        "\"\"\"\n",
        "val = nltk.Valuation.fromstring(v)\n",
        "\n",
        "g = nltk.Assignment(val.domain)\n",
        "\n",
        "m = nltk.Model(val.domain, val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k76GFOYHxv8"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os-gG_6OvKPv"
      },
      "source": [
        "# Define grammar\n",
        "\n",
        "# Mostly Jess, Some Joe#\n",
        "\n",
        "gs = r\"\"\"\n",
        "\n",
        "% start S\n",
        "############################\n",
        "# Grammar Rules\n",
        "#############################\n",
        "S[SEM = <?s>] -> WHP S[SEM=?s]/NP\n",
        "S[SEM = <?v(x)>] -> WHP VP[SEM = ?v]/NP\n",
        "S[SEM = <?v(x)>] -> WHP VP[SEM = ?v]\n",
        "\n",
        "TV[VC=pass,SEM=<(\\R Q z.Q(\\w. (((R(\\T.T))(w))(z))))(?tv)>] -> TV[VC=act,SEM=?tv]\n",
        "# DTV[VC=pass,SEM=<(\\R Q z y.Q(\\w. (((R(\\T.T))(w))(y)(z))))(?dtv)>] -> DTV[VC=act,SEM=?dtv]\n",
        "\n",
        "\n",
        "# wh-questions with gaps\n",
        "S[SEM = <?subj(?vp)>]/NP -> AUX NP[SEM=?subj] VP[SEM=?vp]/NP\n",
        "S[SEM = <?subj(?vp)>] -> AUX NP[SEM=?subj] VP[SEM=?vp]\n",
        "S[SEM = <?subj(?vp)>] -> NP[SEM=?subj] VP[SEM=?vp]\n",
        "S[SEM = <\\x.(?subj(?vp(x)))>]/NP -> NP[SEM =?subj] VP[SEM=?vp]/NP\n",
        "\n",
        "VP[VC=act,SEM=<?v(?obj)>] -> TV[VC=act,SEM=?v] NP[SEM=?obj]\n",
        "VP[VC=pass,SEM=<?v(?obj)>] -> TV[VC=pass,SEM=?v] PP[PFORM=passive_by,SEM=?obj]\n",
        "VP[VC=pass,SEM=<?v(?obj)>] -> TV[VC=pass,SEM=?v] PP[PFORM=dative_to,SEM=?obj]\n",
        "VP[VC=pass,SEM=<?v(?obj)>] -> TV[VC=pass,SEM=?v] PP[PFORM=for,SEM=?obj]\n",
        "\n",
        "\n",
        "VP[SEM=<?vp>] -> BE VP[VC=pass,SEM=?vp]\n",
        "VP[SEM=?v] -> IV[SEM=?v]\n",
        "VP[SEM=<?v(?obj,?pp)>] -> DTV[SEM=?v] NP[SEM=?obj] PP[SEM=?pp]\n",
        "VP[SEM=<?v(?obj,\\P.P(w))>]/NP -> DTV[SEM=?v] NP[SEM=?obj] PP[PFORM=passive_by]/NP\n",
        "VP[SEM=<?v(?obj,\\P.P(w))>]/NP -> DTV[SEM=?v] NP[SEM=?obj] PP[PFORM=for]/NP\n",
        "VP[SEM=<\\x.(?v(?obj,\\P.P(x)))>]/NP -> DTV[SEM=?v] NP[SEM=?obj] PP[PFORM=dative_to]/NP\n",
        "VP[SEM=<?v(?obj)>] -> TV[SEM=?v] NP[SEM=?obj]\n",
        "VP[SEM=<(\\R Q z.Q(\\w. (((R(\\T.T))(w))(z))))(?v)(\\P.P(w))>]/NP -> TV[VC=act,SEM=?v] PP[PFORM=passive_by]/NP\n",
        "\n",
        "VP[SEM=<?v(\\P.P(w))>]/NP -> TV[SEM=?v,VC=act]\n",
        "VP[SEM=<?v(\\P.P(w))>]/NP -> TV[SEM=?v,VC=pass] PP[PFORM=passive_by]/NP\n",
        "\n",
        "PP[PFORM=n_a,SEM=<\\x.?p(?np, x)>] -> P[PFORM=n_a,SEM=?p]  NP[SEM=?np]\n",
        "PP[PFORM=passive_by,SEM=?np] -> P[PFORM=passive_by]  NP[SEM=?np]\n",
        "\n",
        "PP[PFORM=dative_to,SEM=?np] -> P[PFORM=dative_to] NP[SEM=?np]\n",
        "PP[PFORM=dative_to,SEM=?np] -> P[PFORM=for] NP[SEM=?np]\n",
        "PP[PFORM=dative_to]/NP -> P[PFORM=dative_to]\n",
        "PP[PFORM=for]/NP -> P[PFORM=for]\n",
        "PP[PFORM=passive_by]/NP -> P[PFORM=passive_by]\n",
        "\n",
        "\n",
        "NP[SEM=<?det(?nom)>] -> DT[SEM=?det]  N[SEM=?nom]\n",
        "NP[SEM=?np] -> PN[SEM=?np]\n",
        "NP[SEM=?np] -> NP[SEM=?np] CP NP VP/NP\n",
        "\n",
        "\n",
        "ADJ[SEM=?jj] -> ADJ[SEM=?jj] ADJ[SEM=?jj]\n",
        "N[SEM=<?jj(?nom)>] -> ADJ[SEM=?jj] N[SEM=?nom]\n",
        "N[SEM=<?pp(?nom)>] -> N[SEM=?nom] PP[PFORM=n_a,SEM=?pp]\n",
        "\n",
        "CP[SEM=<?s>] -> C S[SEM = ?s]/NP\n",
        "CP[SEM=<?vp>] -> C VP[SEM=?vp]\n",
        "N[SEM=<\\x.(?nom(x) & ?cp(x))>] -> N[SEM=?nom] CP[SEM=?cp]\n",
        "N[SEM=<\\x.(?nom(x) & ?cp(x))>] -> N[SEM=?nom] CP[SEM=?cp]/NP\n",
        "\n",
        "#############################\n",
        "# Lexical entries\n",
        "#############################\n",
        "\n",
        "AUX -> 'do'\n",
        "AUX -> 'should'\n",
        "AUX -> 'be'\n",
        "\n",
        "WHP -> 'which'\n",
        "WHP -> 'who'\n",
        "WHP -> 'what'\n",
        "WHP -> 'where'\n",
        "\n",
        "C -> 'that'\n",
        "\n",
        "FOR -> 'for'\n",
        "BE -> 'be' \n",
        "P[PFORM=passive_by] -> 'by'\n",
        "\n",
        "\n",
        "P[PFORM=n_a, SEM=<\\P Q x.(Q(x) & P(\\y.with(x,y)))>] -> 'with'\n",
        "P[PFORM=n_a, SEM=<\\P Q x.(Q(x) & P(\\y.around(x,y)))>] -> 'around'\n",
        "P[PFORM=n_a, SEM=<\\P Q x.(Q(x) & P(\\y.in(x,y)))>] -> 'in'\n",
        "P[PFORM=n_a, SEM=<\\P Q x.(Q(x) & P(\\y.near(x,y)))>] -> 'near'\n",
        "P[PFORM=n_a, SEM=<\\P Q x.(Q(x) & P(\\y.under(x,y)))>] -> 'under'\n",
        "P[PFORM=n_a, SEM=<\\P Q x.(Q(x) & P(\\y.over(x,y)))>] -> 'over'\n",
        "P[PFORM=n_a, SEM=<\\P Q x.(Q(x) & P(\\y.on(x,y)))>] -> 'on'\n",
        "\n",
        "P[PFORM=dative_to] -> 'to'\n",
        "P[PFORM=for] -> 'for'\n",
        "# P[PFORM=for] -> 'with'\n",
        "\n",
        "\n",
        "DT[SEM=<\\P Q.all x.(P(x) -> Q(x))>] -> 'every'\n",
        "DT[SEM=<\\P Q.all x.(P(x) -> Q(x))>] -> 'all'\n",
        "DT[SEM=<\\P Q.exists x.(P(x) & Q(x))>] -> 'some'\n",
        "DT[SEM=<\\P Q.exists x.(P(x) & Q(x))>] -> 'a'\n",
        "DT[SEM=<\\P Q.exists x.(P(x) & Q(x))>] -> 'an'\n",
        "DT[SEM=<\\P Q.exists x.(P(x) & Q(x))>] -> 'the'\n",
        "\n",
        "\n",
        "ADJ[SEM=<\\P x.(P(x) & tall(x))>] -> 'tall'\n",
        "ADJ[SEM=<\\P x.(P(x) & skinny(x))>] -> 'skinny'\n",
        "ADJ[SEM=<\\P x.(P(x) & black(x))>] -> 'black'\n",
        "ADJ[SEM=<\\P x.(P(x) & drunk(x))>] -> 'drunk'\n",
        "ADJ[SEM=<\\P x.(P(x) & fat(x))>] -> 'fat'\n",
        "ADJ[SEM=<\\P x.(P(x) & blue(x))>] -> 'blue'\n",
        "ADJ[SEM=<\\P x.(P(x) & stinky(x))>] -> 'stinky'\n",
        "ADJ[SEM=<\\P x.(P(x) & sad(x))>] -> 'sad'\n",
        "ADJ[SEM=<\\P x.(P(x) & lovely(x))>] -> 'lovely'\n",
        "ADJ[SEM=<\\P x.(P(x) & polite(x))>] -> 'polite'\n",
        "ADJ[SEM=<\\P x.(P(x) & adaptable(x))>] -> 'adaptable'\n",
        "ADJ[SEM=<\\P x.(P(x) & scary(x))>] -> 'scary'\n",
        "ADJ[SEM=<\\P x.(P(x) & dazzling(x))>] -> 'dazzling'\n",
        "ADJ[SEM=<\\P x.(P(x) & cold(x))>] -> 'cold'\n",
        "\n",
        "PN[SEM=<\\P.P(Robin)>] -> 'robin'\n",
        "PN[SEM=<\\P.P(Mia)>] -> 'mia'\n",
        "PN[SEM=<\\P.P(Sam)>] -> 'sam'\n",
        "PN[SEM=<\\P.P(Tom)>] -> 'tom'\n",
        "PN[SEM=<\\P.P(Jack)>] -> 'jack'\n",
        "PN[SEM=<\\P.P(Andrew)>] -> 'andrew'\n",
        "PN[SEM=<\\P.P(Dave)>] -> 'dave'\n",
        "PN[SEM=<\\P.P(Jess)>] -> 'jess'\n",
        "PN[SEM=<\\P.P(Mike)>] -> 'mike'\n",
        "PN[SEM=<\\P.P(Pumpkin)>] -> 'pumpkin'\n",
        "#This is my cats name so thats why its a proper name :) \n",
        "PN[SEM=<\\P.P(Kate)>] -> 'kate'\n",
        "PN[SEM=<\\P.P(Lance)>] -> 'lance'\n",
        "PN[SEM=<\\P.P(Sheba)>] -> 'sheba'\n",
        "PN[SEM=<\\P.P(Bella)>] -> 'bella'\n",
        "PN[SEM=<\\P.P(Booker)>] -> 'booker'\n",
        "PN[SEM=<\\P.P(Leon)>] -> 'leon'\n",
        "PN[SEM=<\\P.P(Claire)>] -> 'claire'\n",
        "PN[SEM=<\\P.P(Joe)>] -> 'joe'\n",
        "PN[SEM=<\\P.P(Tim)>] -> 'tim'\n",
        "PN[SEM=<\\P.P(Mary)>] -> 'mary'\n",
        "PN[SEM=<\\P.P(Felix)>] -> 'felix'\n",
        "PN[SEM=<\\P.P(Alex)>] -> 'alex'\n",
        "PN[SEM=<\\P.P(Tamika)>] -> 'tamika'\n",
        "PN[SEM=<\\P.P(Gru)>] -> 'gru'\n",
        "\n",
        "N[SEM=<\\x.man(x)>] -> 'man'\n",
        "N[SEM=<\\x.girl(x)>] -> 'girl'\n",
        "N[SEM=<\\x.boy(x)>] -> 'boy'\n",
        "N[SEM=<\\x.dog(x)>] -> 'dog'\n",
        "N[SEM=<\\x.cat(x)>] -> 'cat'\n",
        "N[SEM=<\\x.sedan(x)>] -> 'sedan'\n",
        "N[SEM=<\\x.brochure(x)>] -> 'brochure'\n",
        "N[SEM=<\\x.sailor(x)>] -> 'sailor'\n",
        "N[SEM=<\\x.moustache(x)>] -> 'moustache'\n",
        "N[SEM=<\\x.customer(x)>] -> 'customer'\n",
        "N[SEM=<\\x.dealership(x)>] -> 'dealership'\n",
        "N[SEM=<\\x.corner(x)>] -> 'corner'\n",
        "N[SEM=<\\x.restaurant(x)>] -> 'restaurant'\n",
        "N[SEM=<\\x.computert(x)>] -> 'computer'\n",
        "N[SEM=<\\x.unicorn(x)>] -> 'unicorn'\n",
        "N[SEM=<\\x.dog(x)>] -> 'dog'\n",
        "N[SEM=<\\x.parrot(x)>] -> 'parrot'\n",
        "N[SEM=<\\x.carrot(x)>] -> 'carrot'\n",
        "N[SEM=<\\x.box(x)>] -> 'box'\n",
        "N[SEM=<\\x.notebook(x)>] -> 'notebook'\n",
        "N[SEM=<\\x.shirt(x)>] -> 'shirt'\n",
        "N[SEM=<\\x.smoothie(x)>] -> 'smoothie'\n",
        "N[SEM=<\\x.book(x)>] -> 'book'\n",
        "N[SEM=<\\x.soup(x)>] -> 'soup'\n",
        "N[SEM=<\\x.bird(x)>] -> 'bird'\n",
        "N[SEM=<\\x.guitar(x)>] -> 'guitar'\n",
        "N[SEM=<\\x.theorbo(x)>] -> 'theorbo' \n",
        "N[SEM=<\\x.suspect(x)>] -> 'suspect'\n",
        "N[SEM=<\\x.foreigner(x)>] -> 'foreigner'\n",
        "N[SEM=<\\x.coffee(x)>] -> 'coffee'\n",
        "N[SEM=<\\x.tea(x)>] -> 'tea'\n",
        "N[SEM=<\\x.virus(x)>] -> 'virus'\n",
        "N[SEM=<\\x.staircase(x)>] -> 'staircase'\n",
        "N[SEM=<\\x.phone(x)>] -> 'phone'\n",
        "N[SEM=<\\x.lotion(x)>] -> 'lotion'\n",
        "N[SEM=<\\x.woman(x)>] -> 'woman'\n",
        "N[SEM=<\\x.meme(x)>] -> 'meme'\n",
        "N[SEM=<\\x.woman(x)>] -> 'anime'\n",
        "N[SEM=<\\x.music(x)>] -> 'music'\n",
        "N[SEM=<\\x.motorcycle(x)>] -> 'motorcycle'\n",
        "N[SEM=<\\x.blanket(x)>] -> 'blanket'\n",
        "N[SEM=<\\x.camera(x)>] -> 'camera'\n",
        "N[SEM=<\\x.suit(x)>] -> 'suit'\n",
        "N[SEM=<\\x.wig(x)>] -> 'wig'\n",
        "N[SEM=<\\x.camera(x)>] -> 'camera'\n",
        "N[SEM=<\\x.poster(x)>] -> 'poster'\n",
        "N[SEM=<\\x.popcorn(x)>] -> 'popcorn'\n",
        "N[SEM=<\\x.glitter(x)>] -> 'glitter'\n",
        "N[SEM=<\\x.perfume(x)>] -> 'perfume'\n",
        "N[SEM=<\\x.ramen(x)>] -> 'ramen'\n",
        "N[SEM=<\\x.car(x)>] -> 'car'\n",
        "\n",
        "\n",
        "IV[SEM=<\\x.sneeze(x)>] -> 'sneeze'\n",
        "IV[SEM=<\\x.walk(x)>] -> 'walk'\n",
        "IV[SEM=<\\x.leave(x)>] -> 'leave'\n",
        "IV[SEM=<\\x.leave(x)>] -> 'left'\n",
        "IV[SEM=<\\x.laugh(x)>] -> 'laugh'\n",
        "IV[SEM=<\\x.die(x)>] -> 'die'\n",
        "IV[SEM=<\\x.cry(x)>] -> 'cry'\n",
        "IV[SEM=<\\x.sing(x)>] -> 'sing'\n",
        "IV[SEM=<\\x.sleep(x)>] -> 'sleep'\n",
        "IV[SEM=<\\x.sneeze(x)>] -> 'sneeze'\n",
        "IV[SEM=<\\x.lie(x)>] -> 'lie'\n",
        "\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.interrupt(x,y))>] -> 'interrupt'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.insult(x,y))>] -> 'insult'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.buy(x,y))>] -> 'buy'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.chase(x,y))>] -> 'chase'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.leave(x,y))>] -> 'leave'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.leave(x,y))>] -> 'left'\n",
        "#because spacy tokens this as the direction left it has to be separately in the lexical terms\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.kiss(x,y))>] -> 'kiss'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.see(x,y))>] -> 'see'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.bite(x,y))>] -> 'bite'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.drink(x,y))>] -> 'drink'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.like(x,y))>] -> 'like'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.eat(x,y))>] -> 'eat'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.wear(x,y))>] -> 'wear'\n",
        "TV[VC=act,SEM=<\\P x.P(\\y.sing(x,y))>] -> 'sing'\n",
        "\n",
        "\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.give(x,y,z)))>] -> 'give'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.buy(x,y,z)))>] -> 'buy'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.send(x,y,z)))>] -> 'send'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.tell(x,y,z)))>] -> 'tell'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.show(x,y,z)))>] -> 'show'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.promise(x,y,z)))>] -> 'promise'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.build(x,y,z)))>] -> 'build'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.make(x,y,z)))>] -> 'make'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.prescribe(x,y,z)))>] -> 'prescribe'\n",
        "DTV[VC=act,SEM=<\\P Q x.Q(\\z.P(\\y.sell(x,y,z)))>] -> 'sell'\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load grammar\n",
        "grammar2 = nltk.grammar.FeatureGrammar.fromstring(gs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7L0ku7lH4v0"
      },
      "source": [
        "## Q/A System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V11NwQHYk35Y"
      },
      "source": [
        "# Joe #\n",
        "WHP = [\n",
        "       'who',\n",
        "       'what',\n",
        "       'where',\n",
        "       'which'\n",
        "]\n",
        "NoneAnswerDict = {\n",
        "    'who': 'Nobody',\n",
        "    'what': 'Nothing',\n",
        "    'where': 'Nowhere',\n",
        "    'which': 'None'\n",
        "}\n",
        "def isWH(semTree):\n",
        "  for wh in WHP:\n",
        "    if(wh in semTree.leaves()):\n",
        "      return wh\n",
        "  return False\n",
        "\n",
        "def evaluateSentence(text1):\n",
        "  # Jess and Joe \n",
        "  text = text1.lower() # makes all lowercase so it can be handled by grammar\n",
        "\n",
        "\n",
        "  #Lemmatizing\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(text)\n",
        "  #Joe\n",
        "  stems = [token.lemma_ if(token.lemma_ != '-PRON-') else token.text for token in doc] #this also tokenizes at same time\n",
        "  # print([token.pos_ for token in doc])\n",
        "\n",
        "  stems.pop()\n",
        "  #removes punctuation; since Rui made requirement that statement has to have it.\n",
        "  # print(stems)\n",
        "\n",
        "  # # Select parser\n",
        "  # parser = nltk.parse.FeatureChartParser(grammar2)\n",
        "\n",
        "  # # Find all semantic representations for all parses \n",
        "  # print(\"Active:\")\n",
        "  # for tree in parser.parse(stems):\n",
        "  #     print(tree)\n",
        "  #     print('\\n Meaning: ',tree.label()['SEM'])\n",
        "  \n",
        "  sent = ' '.join(stems)\n",
        "  results = nltk.evaluate_sents([sent], grammar2, m, g)[0]\n",
        "  for (syntree, semrep, value) in results:\n",
        "      tree = syntree\n",
        "      rep = semrep\n",
        "      truth = value\n",
        "  \n",
        "  print(tree) ###### Uncomment to See parse tree. #######\n",
        "  print(rep) ###### Uncomment to See semantic Meaning of Questions. #######\n",
        "  # print(truth)\n",
        "  # Jess and Joe #\n",
        "  print(text1, end=': ')\n",
        "  whQ = isWH(tree)\n",
        "  if \".\" in text1:\n",
        "    if(truth == \"Undefined\"):\n",
        "      print(\"Im not sure.\")\n",
        "    else:\n",
        "      print(\"I know.\") if(truth) else print(\"I dont think so.\")\n",
        "  elif whQ:\n",
        "    formula = rep\n",
        "    freevar = list(formula.free())[0]\n",
        "    satisfiers = list(m.satisfiers(formula, freevar, g))\n",
        "    length = len(satisfiers)\n",
        "    invValNames = {b:a for a,b in val.items() if isinstance(b, str)}\n",
        "    if(length < 1):\n",
        "      print(NoneAnswerDict[whQ])\n",
        "    else:\n",
        "      if(satisfiers[0] == \"uk\"):\n",
        "        print(\"Sorry, I dont know their name.\")\n",
        "      else:\n",
        "        for i in range(0,length - 2):\n",
        "          print(invValNames[satisfiers[i]],end=', ')\n",
        "        if(length > 1):\n",
        "          print(invValNames[satisfiers[length - 2]],end=' and ') \n",
        "        print(invValNames[satisfiers[length - 1]])\n",
        "  else:\n",
        "    if(truth == \"Undefined\"):\n",
        "      print(\"Im not sure.\")\n",
        "    else:\n",
        "      print(\"Yes.\") if(truth) else print(\"No.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWu3POAzHwRe"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji4cfF0y3DHZ"
      },
      "source": [
        "## I/O:\n",
        "\n",
        "In the above cell, uncomment print(rep) to view the semantic meaning of questions that we parsed, and print(tree) to view the parse tree. (Both Marked by Comments to the right of them.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJQOjiex3B4T",
        "outputId": "d579cd21-0783-4cff-db8e-d60af22f7063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "evaluateSentence(\"A tall skinny man sneezed.\") #Passed\n",
        "evaluateSentence(\"Mia gave a brochure to the customer.\") #Passed\n",
        "evaluateSentence(\"Did a tall skinny man sneeze?\") #Passed\n",
        "evaluateSentence(\"Who sneezed?\") #Passed\n",
        "evaluateSentence(\"Who did Sam buy a black sedan in the dealership around the corner for?\") #Passed\n",
        "evaluateSentence(\"Who bought a sedan?\") #Passed\n",
        "evaluateSentence(\"What did Mia see?\") #Passed\n",
        "evaluateSentence(\"Tom saw a blue car.\") #Passed\n",
        "evaluateSentence(\"What did Tom see?\") #Passed\n",
        "evaluateSentence(\"Did Tom see a blue car?\") #Passed\n",
        "evaluateSentence(\"A man left the restaurant.\") #Passed\n",
        "evaluateSentence(\"A cat left the restaurant.\") #Passed\n",
        "evaluateSentence(\"Did a man leave the restaurant?\") #Passed\n",
        "evaluateSentence(\"Did a cat left the restaurant?\") #Passed\n",
        "evaluateSentence(\"Who left the restaurant?\") #Passed\n",
        "evaluateSentence(\"What did Mary see?\") #Passed\n",
        "evaluateSentence(\"Sam bought a black sedan in the dealership around the corner.\") #Passed\n",
        "evaluateSentence(\"The customer that Mia gave a brochure to left the dealership.\") #Passed\n",
        "evaluateSentence(\"A blue car was seen by Tom.\") #Passed\n",
        "evaluateSentence(\"What was seen by Tom?\") #Passed\n",
        "evaluateSentence(\"Was a blue car seen by Tom?\") #Passed\n",
        "evaluateSentence(\"Robin bought a car that was seen by Tom.\") #Passed\n",
        "evaluateSentence(\"Robin was interrupted by a drunk sailor with a moustache.\") #Passed\n",
        "evaluateSentence(\"Did Sam buy a black sedan in the dealership around the corner for Mia?\") #Passed\n",
        "evaluateSentence(\"Did the customer that Mia gave a brochure to leave the dealership?\") #Passed\n",
        "evaluateSentence(\"Was Robin interrupted by a drunk sailor with a moustache?\") #Passed\n",
        "evaluateSentence(\"Sam bought a sedan for Robin.\") #Passed\n",
        "evaluateSentence(\"What did the customer that Mia gave a brochure to leave?\") #Passed\n",
        "evaluateSentence(\"Who did Sam buy a sedan for?\") #Passed\n",
        "evaluateSentence(\"What was Robin interrupted by?\") #Passed\n",
        "evaluateSentence(\"Was Robin interrupted by Joe?\") #Passed\n",
        "evaluateSentence(\"What was Alex interrupted by?\") #Passed\n",
        "evaluateSentence(\"Who Did the customer that Mia gave a brochure to insult?\") #Passed\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S[SEM=<exists x.(man(x) & skinny(x) & tall(x) & sneeze(x))>]\n",
            "  (NP[SEM=<\\Q.exists x.(man(x) & skinny(x) & tall(x) & Q(x))>]\n",
            "    (DT[SEM=<\\P Q.exists x.(P(x) & Q(x))>] a)\n",
            "    (N[SEM=<\\x.(man(x) & skinny(x) & tall(x))>]\n",
            "      (ADJ[SEM=<\\P x.(P(x) & tall(x))>] tall)\n",
            "      (N[SEM=<\\x.(man(x) & skinny(x))>]\n",
            "        (ADJ[SEM=<\\P x.(P(x) & skinny(x))>] skinny)\n",
            "        (N[SEM=<\\x.man(x)>] man))))\n",
            "  (VP[SEM=<\\x.sneeze(x)>] (IV[SEM=<\\x.sneeze(x)>] sneeze)))\n",
            "exists x.(man(x) & skinny(x) & tall(x) & sneeze(x))\n",
            "A tall skinny man sneezed.: I know.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
